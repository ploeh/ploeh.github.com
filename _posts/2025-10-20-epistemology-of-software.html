---
layout: post
title: "Epistemology of software"
description: "How do you know that your code works?"
date: 2025-10-20 6:16 UTC
tags: [Productivity, Unit Testing]
image: "/content/binary/science-of-tdd-second-test-just-enough-production-code.png"
image_alt: "Two boxes labelled 'production code' and 'test code'. The test-code box contains a single blue arrow going from a to b, and red arrow going from a to g. The product code box contains green arrows going from a to b to c, and other arrows branching off from b to go to g via e."
---
{% include JB/setup %}

<div id="post">
    <p>
        <em>{{ page.description }}</em>
    </p>
    <p>
        In 2023 I gave a conference keynote titled <em>Epistemology of software</em>, a recording of which is <a href="https://youtu.be/bLpwNWWs5EY?si=SwLJ_H6WtQDEDayK">available on YouTube</a>. In it, I try to answer the question: How do we know that software works?
    </p>
    <p>
        The keynote was for a mixed audience with some technical, but also a big contingent of non-technical, software people, so I took a long detour around general <a href="https://en.wikipedia.org/wiki/Epistemology">epistemology</a>, and particularly the philosophy of science as it pertains to empirical science. Towards the end of the presentation, I returned to the epistemology of software in particular. While I recommend that you watch the recording for a broader perspective, I want to reiterate the points about software development here. Personally, I like prose better than video when it comes to succinctly present and preserve ideas.
    </p>
    <h3 id="1416a12dc0a745fa99c643856b709975">
        How do we know anything? <a href="#1416a12dc0a745fa99c643856b709975">#</a>
    </h3>
    <p>
        In philosophy of science it's long been an established truth that we can't know anything with certainty. We can only edge asymptotically closer to what we believe is the 'truth'. The most effective method for that is the 'scientific method', which grossly simplified is an iterative process of forming hypotheses, making predictions, performing experiments, and corroborating or falsifying predictions. When experiments don't quite turn out as predicted, you may adjust your hypothesis accordingly.
    </p>
    <p>
        <img src="/content/binary/prediction-experiment-adjustment-cycle.png" alt="Cycle with arrows from prediction to experiment, from experiment to adjustment, and from adjustment to prediction." width="400">
    </p>
    <p>
        An example, however, may still be useful to set the stage. Consider <a href="https://en.wikipedia.org/wiki/Galileo_Galilei">Galilei</a>'s idea of dropping a small and a big ball from the <a href="https://en.wikipedia.org/wiki/Leaning_Tower_of_Pisa">Leaning Tower of Pisa</a>. The prediction is that two objects of different mass will hit the ground simultaneously, if dropped from the same height simultaneously. This experiment have been carried out multiple times, even <a href="https://youtu.be/E43-CfukEgs?si=kkg-Gxscr-7UUfM7">in vacuum chambers</a>.
    </p>
    <p>
        Even so, thousands of experiments do not constitute <em>proof</em> that two objects fall with the same acceleration. The experiments only make it exceedingly likely that this is so.
    </p>
    <p>
        What happens if you apply scientific thinking to software development?
    </p>
    <h3 id="77040a20e7804b43ba220a3332813a16">
        Empirical epistemology of software <a href="#77040a20e7804b43ba220a3332813a16">#</a>
    </h3>
    <p>
        Put yourself in the shoes of a non-coding product owner. He or she needs an application to solve a particular problem. How does he or she know when that goal has been reached?
    </p>
    <p>
        Ultimately, he or she can only do what any scientist can do: Form hypotheses, make predictions, and perform experiments. Sometimes product owners assign these jobs to other people, but so do scientists.
    </p>
    <p>
        <img src="/content/binary/write-software-test-shatter-all-illusions-cycle.png" alt="Cycle with arrows from write software to test, from test to shatter all illusions, and from shatter all illusions to write software." width="400">
    </p>
    <p>
        Testing can be ad-hoc or planned, automated or manual, thorough or cursory, but it's really the only way a product owner can determine whether or not the software works.
    </p>
    <p>
        When I started as a software developer, testing was often the purview of a dedicated testing department. The test manager would oversee the production of a <em>test plan</em>, which was a written document that manual testers were supposed to follow. Even so, this, too, is an empirical approach to software verification. Each test essentially implies a hypothesis: If we perform these steps, the application will behave in this particular, observable manner.
    </p>
    <p>
        For an application designed for human interaction, letting a real human tester interact with it may be the most realistic test scenario, but human testers are slow. They also tend to make mistakes. The twentieth time they run through the same test scenario, they are likely to overlook details.
    </p>
    <p>
        If you want to perform faster, and more reliable, tests, you may wish to automated the tests. You could, for example, write code that executes a test plan. This, however, raises another problem: If tests are also code, how do we know that the tests contain no bugs?
    </p>
    <h3 id="346c4408124b4fa7ad16fe5a5aa37bb2">
        The scientific method of software <a href="#346c4408124b4fa7ad16fe5a5aa37bb2">#</a>
    </h3>
    <p>
        How about using the the scientific method? Even more specifically, proceed by making incremental <a href="https://en.wikipedia.org/wiki/Falsifiability">falsifiable</a> predictions about the code you write.
    </p>
    <p>
        For instance, write a single automated test without accompanying production code:
    </p>
    <p>
        <img src="/content/binary/science-of-tdd-first-test-no-production-code.png" alt="Two boxes labelled 'production code' and 'test code'. The test-code box contains a single red arrow going from a to b." width="500">
    </p>
    <p>
        Before you run the test code, you make a prediction based on the implicit hypothesis formulated by the <a href="/2019/10/21/a-red-green-refactor-checklist">red-green-refactor checklist</a>: <em>If this test runs, it will fail</em>.
    </p>
    <p>
        This is a falsifiable prediction. While you expect the test to fail, it may succeed if, for example, you've inadvertently written a <a href="/2019/10/14/tautological-assertion">tautological assertion</a>. In other words, if your prediction is falsified, you know that the test code is somehow wrong. On the other hand, if the test fails (as predicted), you've failed to falsify your prediction. As empirical science goes, this is the best you can hope for. It doesn't prove that the test is correct, but corroborates the hypothesis that it is.
    </p>
    <p>
        The next step in the red-green-refactor cycle is to write just enough code to pass the test. You do that, and before rerunning the test implicitly formulate the opposite hypothesis: <em>If this test runs, it will succeed</em>.
    </p>
    <p>
        <img src="/content/binary/science-of-tdd-first-test-just-enough-production-code.png" alt="Two boxes labelled 'production code' and 'test code'. The test-code box contains a single red arrow going from a to b. The product code box contains green arrows going from a to b to c." width="500">
    </p>
    <p>
        This, again, is a falsifiable prediction. If, despite expectations, the test fails, you know that something is wrong. Most likely, it's the implementation that you just wrote, but it could also be the test which, after all, is somehow defective. Or perhaps a circuit in your computer was struck by a cosmic ray. On the other hand, if the test passes, you've failed to falsify your prediction, which is the best you can hope for.
    </p>
    <p>
        You now write a second test, which comes with the implicit falsifiable prediction: <em>If I run all tests, the new test will fail</em>.
    </p>
    <p>
        <img src="/content/binary/science-of-tdd-second-test-no-new-production-code.png" alt="Two boxes labelled 'production code' and 'test code'. The test-code box contains a single blue arrow going from a to b, and red arrow going from a to g. The product code box contains green arrows going from a to b to c." width="500">
    </p>
    <p>
        The process repeats. A succeeding test falsifies the prediction, while a failing test only corroborates the hypothesis.
    </p>
    <p>
        Again, implement just enough code for the hypothesis that if you run all tests, they will pass.
    </p>
    <p>
        <img src="/content/binary/science-of-tdd-second-test-just-enough-production-code.png" alt="Two boxes labelled 'production code' and 'test code'. The test-code box contains a single blue arrow going from a to b, and red arrow going from a to g. The product code box contains green arrows going from a to b to c, and other arrows branching off from b to go to g via e." width="500">
    </p>
    <p>
        If this hypothesis, too, is corroborated (i.e. you failed to falsify it), you move on until you believe that you're done.
    </p>
    <p>
        <img src="/content/binary/science-of-tdd-third-test-just-enough-production-code.png" alt="Two boxes labelled 'production code' and 'test code'. The test-code box contains blue arrows going from a to b, from a to g, and another arrow going from a to h. The product code box contains green arrows going from a to b to c, and other arrows branching off from b to go to g via e, and yet another set of arrows going from a to d to f to h." width="500">
    </p>
    <p>
        As this process proceeds, you corroborate two related hypotheses: That the test code is correct, and that the production code is. None of these hypotheses are ever proven, but as you add tests that are first red, then green, and then stay green, you increase confidence that the entire code complex works as intended.
    </p>
    <p>
        If you don't write the test first, you don't get to perform the first experiment: That you predict the new test to fail. If you don't do that, you collect no empirical evidence that the tests work as hypothesized. In other words, you'd lose half of the scientific evidence you otherwise could have gathered.
    </p>
    <h3 id="119d07352baa4bb38627329eede93654">
        TDD is the scientific method <a href="#119d07352baa4bb38627329eede93654">#</a>
    </h3>
    <p>
        Let me spell this out: Test-driven development (TDD) is an example of the scientific method. Watching a new test fail is an important part of the process. Without it, you have no empirical reason to believe that the tests are correct.
    </p>
    <p>
        While you may <em>read</em> the test code, that only puts you on the same scientific footing as the ancient Greeks' introspective philosophy: <a href="https://en.wikipedia.org/wiki/Humorism">The four humours</a>, <a href="https://en.wikipedia.org/wiki/Emission_theory_(vision)">extramission</a>, <a href="https://en.wikipedia.org/wiki/Classical_element">the elements</a>, etc. By reading test code, or even writing it, you may believe that you understand what the code does, but reading it without running it gives you no empirical way to verify whether that belief is correct.
    </p>
    <p>
        Consider: How many times have you written code that you believed was correct, but turned out contained errors?
    </p>
    <p>
        If you write the test <em>after</em> the system under test (SUT), you can run the test to see it pass, but consider what you can only falsify in that case: If the test passes, you've learned little. It <em>may</em> be that the test exercises the SUT, but it may also be that you've written a <a href="/2019/10/14/tautological-assertion">tautological assertion</a>. It may also be that you've faithfully captured a bug in the production code, and now preserved in for eternity as something that looks like a regression test. Or perhaps the test doesn't even cover the code path that you believe it covers.
    </p>
    <p>
        Conversely, if such a test (that you believe to be correct) fails, you're also in the dark. Was the test wrong, after all? Or does the SUT have a defect?
    </p>
    <p>
        This is the reason that the process for writing <a href="https://en.wikipedia.org/wiki/Characterization_test">Characterization Tests</a> includes a step where you
    </p>
    <blockquote>
        <p>
            "Write an assertion that you know will fail."
        </p>
        <footer><cite><a href="/ref/wewlc">Working Effectively with Legacy Code</a></cite>, Michael Feathers, Prentice Hall, 2005</footer>
    </blockquote>
    <p>
        I prefer a variation where I write what I believe is the correct assertion, but then temporarily sabotage the SUT to fail the assertion. The important part is to see the test fail, because the failure to falsify a strong prediction is important empirical evidence.
    </p>
    <h3 id="528753dd0ff844c9a61ddccf96469be4">
        Conclusion <a href="#528753dd0ff844c9a61ddccf96469be4">#</a>
    </h3>
    <p>
        How do we know that the software we develop works as intended? The answer lies in the much larger question: How do we know anything?
    </p>
    <p>
        Scientific thinking effectively answers this by 'the scientific method': Form a hypothesis, make falsifiable predictions, perform experiments, adjust, repeat.
    </p>
    <p>
        We can subject software to the same rigorous regimen that scientists do: Hypothesize that the software works in certain ways under given conditions, predict observable behaviour, test, record outcomes, fix defects, repeat.
    </p>
    <p>
        Test-driven development closely follows that process, so is a highly scientific methodology for developing software. It should be noted that science is hard, and so is TDD. Still, if you care that your software behaves as it's supposed to, it's one of the most rigorous and effective processes I'm aware of.
    </p>
</div>